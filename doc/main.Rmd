---
title: 'Optical character recognition (OCR)'
output: 
  html_document:
    toc: true
    toc_depth: 2
    toc_float:
      collapsed: false
      smooth_scroll: false
    number_sections: true
    code_folding: hide
---

GU4243/GR5243: Applied Data Science

<style type="text/css">
h1.title {
  font-size: 24px;
  color: Black;
}
h1 { /* Header 1 */
  font-size: 24px;
  color: Black;
}
h2 { /* Header 2 */
  font-size: 20px;
  color: Black;
}
h3 { /* Header 3 */
  font-size: 16px;
  color: Black;
}
h4 { /* Header 4 */
  font-size: 14px;
  color: Grey;
}
</style>
# Introduction {-}

Optical character recognition (OCR) is the process of converting scanned images of machine printed or
handwritten text (numerals, letters, and symbols), into machine readable character streams, plain (e.g. text files) or formatted (e.g. HTML files). As shown in Figure 1, the data *workflow* in a typical OCR system consists of three major stages:

* Pre-processing

* Word recognition

* Post-processing

![](../figs/ocr_flowchart.png) 

We have processed raw scanned images through the first two steps are relying on the [Tessearct OCR machine](https://en.wikipedia.org/wiki/Tesseract_(software)). R package tutorial can be found [here](https://www.r-bloggers.com/the-new-tesseract-package-high-quality-ocr-in-r/). 

BUT this is not the FOCUS of this project!!!

In this project, we are going to **focus on the third stage -- post-processing**, which includes two tasks: *error detection* and *error correction*.  

# Step 1 - Load library and source code:

```{r, warning=FALSE, message = FALSE}
if (!require("devtools")) install.packages("devtools")
if (!require("pacman")) {
  ## devtools is required
  library(devtools)
  install_github("trinker/pacman")
}
# library(lexicon)
library(dplyr)
library(plyr)
library(tidyverse)

pacman::p_load(knitr, readr, stringr, tesseract, vecsets)
source('../lib/ifCleanToken.R')
file_name_vec <- list.files("../data/ground_truth") #100 files in total
```

# Step 2 - Read the files and conduct Tesseract OCR

Although we have processed the Tesseract OCR and save the output txt files in the `data` folder, we include this chunk of code in order to make clear the whole pipeline to you.

```{r, eval=FALSE}
for(i in c(1:length(file_name_vec))){
  current_file_name <- sub(".txt","",file_name_vec[i])
  ## png folder is not provided on github (the code is only for demonstration purpose)
  current_tesseract_txt <- tesseract::ocr(paste("../data/png/",current_file_name,".png",sep=""))
  
  ### clean the tessetact text (separate line by "\n", delete null string, transter to lower case)
  clean_tesseract_txt <- strsplit(current_tesseract_txt,"\n")[[1]]
  clean_tesseract_txt <- clean_tesseract_txt[clean_tesseract_txt!=""]
  
  ### save tesseract text file
  writeLines(clean_tesseract_txt, paste("../data/tesseract/",current_file_name,".txt",sep=""))
}
```

####Data Cleaning:

Later, in order to develop the confusion matrix based on `ground_truth files` and to apply the precision measurement, we need to match `ground_truth` and `tesseract`.

```{r, eval=FALSE}
#set ground_truth file and tesseract file path
ground_truth_path <- "../data/ground_truth_org/"
tesseract_text_path <- "../data/tesseract_org/"
unmatched_file_name <- array("",100)
```

```{r, eval=FALSE}
# Detect if we have files in ground_truth and tesseract that are corresponding to each other but with different line numbers
j <- 1
for (i in c(1:length(file_name_vec))){
  # find the length of each document
  ground_truth_docu <- readLines(paste0(ground_truth_path,file_name_vec[i]), warn = FALSE, encoding = "UTF-8")
  tesseract_text_docu <- readLines(paste0(tesseract_text_path,file_name_vec[i]), warn = FALSE, encoding = "UTF-8")
  if (length(ground_truth_docu) != length(tesseract_text_docu)){
    unmatched_file_name[j] <- file_name_vec[i]
    j <- j + 1
  }
}

# Number of files that have unmatched lines
j-1 # 13 files have unmatched lines

# Name of files that have unmatched lines
unmatched_file_name[1:j-1]
```

By examing the line difference, we notice that it is coming from unmatched space line. We can manually adjust this problem, and from now on, we do not have files with unmatched line numbers. From this point, to avoid any confusion, we rename the original file folders into `ground_truth_org` and `tesseract_org`.

```{r, eval=FALSE}
# Remove the line with different length of tokens
for (i in c(1:length(file_name_vec))){
  ground_truth_docu <- readLines(paste0(ground_truth_path,file_name_vec[i]), warn = FALSE, encoding = "UTF-8")
  tesseract_text_docu <- readLines(paste0(tesseract_text_path,file_name_vec[i]), warn = FALSE, encoding = "UTF-8")
  total_line <-c()
  total_line <- total_line + length(ground_truth_docu)
  temp_ground_docu <- c()
  temp_tesseract_docu <- c()
  for (j in c(1:length(ground_truth_docu))){
    ground_truth_words <- strsplit(ground_truth_docu[j], " ")
    ground_truth_words <- ground_truth_words[[1]]
    tesseract_text_words <- strsplit(tesseract_text_docu[j], " ")
    tesseract_text_words <- tesseract_text_words[[1]]
    
    #check the length of lines
    if (length(ground_truth_words) == length(tesseract_text_words)){
      #get each words length
      ground_nchar <- nchar(ground_truth_words)
      tesseract_nchar <- nchar(tesseract_text_words)
      #find the word with same length
      idx_nchar <- ground_nchar == tesseract_nchar
      #write the line with same words length
      temp_ground_docu <- c(temp_ground_docu, paste0(ground_truth_words[idx_nchar], collapse = " "))
      temp_tesseract_docu <- c(temp_tesseract_docu, paste0(tesseract_text_words[idx_nchar], collapse = " "))
    }
  }
  writeLines(temp_ground_docu, paste0("../output/ground_truth_clean/",file_name_vec[i]), useBytes = TRUE)
  writeLines(temp_tesseract_docu, paste0("../output/tesseract_text_clean/",file_name_vec[i]), useBytes = TRUE)
}
```

We save the cleaned files in `output` so that it will not be confused with the original data folder.

# Step 3 - Error detection

Now, we are ready to conduct post-processing, based on the Tessearct OCR output. First of all, we need to detect errors, or *incorrectly processed words* -- check to see if an input string is a valid dictionary word or if its n-grams are all legal.

The referenced papers are:

1. [Rule-based techniques](http://webpages.ursinus.edu/akontostathis/KulpKontostathisFinal.pdf)

- rules are in the section 2.2 

```{r eval=FALSE}
# store the tesseract_vec into RData
word_list <- function(i) #parameter is the index of the file
{
  current_file_name <- sub(".txt","",file_name_vec[i])
  current_ground_truth_txt <- readLines(paste("../data/ground_truth/",current_file_name,".txt",sep=""), warn=FALSE)
  current_tesseract_txt <- readLines(paste("../data/tesseract/",current_file_name,".txt",sep=""), warn=FALSE)
  clean_tesseract_txt <- paste(current_tesseract_txt, collapse = " ")
  tesseract_vec <- str_split(clean_tesseract_txt," ")[[1]]
  return(tesseract_vec)
}

tesseract_vec <- mapply(word_list,1:length(file_name_vec))
save(tesseract_vec, file = '../output/tesseract_vec.RData')
```

```{r eval=FALSE}
# Detect the error and return a true/false
Error_detection <- function(i) #parameter is the index of the file
{
  current_file_name <- sub(".txt","",file_name_vec[i])
  current_ground_truth_txt <- readLines(paste("../data/ground_truth/",current_file_name,".txt",sep=""), warn=FALSE)
  current_tesseract_txt <- readLines(paste("../data/tesseract/",current_file_name,".txt",sep=""), warn=FALSE)
  clean_tesseract_txt <- paste(current_tesseract_txt, collapse = " ")
  tesseract_vec <- str_split(clean_tesseract_txt," ")[[1]]
  tesseract_if_clean <- unlist(lapply(tesseract_vec,ifCleanToken))
  return(tesseract_if_clean)
}

detection_result <- mapply(Error_detection,1:length(file_name_vec))
save(detection_result, file = '../output/detection_result.RData')
```

# Step 4 - Error correction

Given the detected word error, in order to find the best correction, we need to generating the candidate corrections: a dictionary or a database of legal n-grams to locate one or more potential correction terms. Then we need invoke some lexical-similarity measure between the misspelled string and the candidates or a probabilistic estimate of the likelihood of the correction to rank order the candidates.

The referenced papers are:

4. [Probability scoring with contextual constraints](https://link.springer.com/content/pdf/10.1007%2FBF01889984.pdf)

- focus on section 5

####Step 0: Generate Potential Candidates

Here, we are using `ground_truth` as the dictionary to generate the potential candidates. This `candidates_gd` will be used as the primary source for this on-going project.

```{r, eval=FALSE}
load('../output/typos.RData')
## For each typo, list the candidate
source('../lib/potential_correction_gd.R')

candidates <- data.frame(typo=character(),
                           correction=character(),
                           pos=integer(),
                           change=character(),
                           type=character(),
                           stringsAsFactors=FALSE)

# only need to compute the probability for unique typos
typo_unique <- sapply(typos[,1],tolower)%>%
  as.vector()%>%
  gsub('[[:punct:] ]+','',.)%>%
  as.data.frame(stringsAsFactors=FALSE)%>%
  distinct()

typo_unique <- typo_unique[typo_unique!="",] # remove empty elements
num_letter <- sapply(typo_unique,strsplit,split="")%>%
  sapply(.,length)%>%
  as.vector()
typo_unique <- typo_unique[num_letter>1] # remove empty elements and elements that only have one letter

for(i in 1:length(typo_unique))
{
  temp <- potential_correction(typo_unique[i])
  candidates <- candidates%>%
    bind_rows(temp)
  # if(i%%100==0)
  # {print(i)}  This helps to track the process
}
# Note that some of the typos cannot find a meaningful(listed in the dictionary) correction.
candidates_gd <- candidates
save(candidates_gd,file='../output/candidates_gd.RData') # possible correction by ground_truth files
```

####Step 1: Without Context (Section 3 in assigned paper)

In this step, we are going to calculate $P(c)$ and $P(t|c)$. We use `ground_truth` as the dictionary, combining with the confusion matrix outputed from the paper, to calculate these two probabilities.

```{r, eval=FALSE}
## Step1: Load Frequency Table of Ground Truth
load("../output/ground_truth_frequency.RData")
ground_truth_freq$tokens <- as.character(ground_truth_freq$tokens)

## Step 2: Find the Frequency of Single Character and the Consecutive Two Letters
chars <- array(0, dim=c(27,26))
for(i in 1:nrow(ground_truth_freq)){
  string <- ground_truth_freq$tokens[i]
  count <- ground_truth_freq$n[i]
  head <- substring(string,1,1)
  chars[27,which(letters==head)] =  chars[27,which(letters==head)] + count
  for (s in 2:nchar(string)) {
    x = substring(string,s-1, s-1)
    y = substring(string, s, s)
    row = which(letters==x)
    col = which(letters==y)
    chars[row, col] = chars[row, col] + count
  }
}
# Adding names to the character matrix
# Explanation: the character matrix captures the frequency of a pair of letter showing up in the AP dictionary
# the last row counts the frequency of a single letter
colnames(chars)<-letters
rownames(chars)<-c(letters,"@")
chars
save(chars,file='../output/character_bigram_gd.RData')

## Step 3: Importing the Confusion Matrix
source('../lib/potential_correction_gd.R')
m_add <- read_csv("../data/confusion_matrix/add_matrix.csv") %>% select(2:27)
m_del <- read_csv("../data/confusion_matrix/del_matrix.csv") %>% select(2:27)
m_rev <- read_csv("../data/confusion_matrix/rev_matrix.csv") %>% select(2:27)
m_sub <- read_csv("../data/confusion_matrix/sub_matrix.csv") %>% select(2:27)

## Step 4: Conditional Probability Calculation
add_prob <- function(x, y){
  if(y %in% letters){
    idx_x <- ifelse(x %in% letters, which(letters==x), 27)
    idx_y <- which(letters==y)
    return(m_add[idx_x, idx_y]/sum(chars[idx_x,]))
  }
  else return(0)
}

del_prob <- function(x, y){
  idx_x <- ifelse(x %in% letters, which(letters==x), 27)
  idx_y <- which(letters==y)
  return(m_del[idx_x, idx_y]/chars[idx_x, idx_y])
}

rev_prob <- function(x, y){
  idx_x <- which(letters==x)
  idx_y <- which(letters==y)
  return(m_rev[idx_x, idx_y]/chars[idx_x, idx_y])
}

sub_prob <- function(x, y){
  idx_x <- which(letters==x)
  idx_y <- which(letters==y)
  return(m_sub[idx_x, idx_y]/sum(chars[,idx_y]))
}

## Step 5: Calculate the Prior: pr(c)
### this is the probability of the correct word appearing in the ground_truth dictionary 
prior <- function(c){
  N <- sum(ground_truth_freq[,2])
  V <- nrow(ground_truth_freq)
  freq <- ifelse((c %in% ground_truth_freq$tokens),
                 ground_truth_freq[ground_truth_freq$tokens==c,2],
                 0)
  return((freq+0.5)/(N+V/2))
}

## Step 6: Calculate the conditional (error) probability
load("../output/candidates_gd.RData")
prob_t_c_ap <- function(typo){
  # Input: the wrong word
  # Output: a data frame contains:
  ## - typo: the wrong word typed in
  ## - correction: possible corrections
  ## - pc: prior probability
  ## - ptc: error probability
  list_cand <- potential_correction(typo)
  correction <- c()
  scr <- c()
  pc <- c()
  for (i in 1:nrow(list_cand)) {
    cor <- list_cand[i,]$correction
    typ <- list_cand[i,]$type
    t <- list_cand[i,]$typo
    pos <- list_cand[i,]$pos
    chn <- list_cand[i,]$change
    
    if(typ=='i'){
      s <- as.numeric(ifelse(pos==0, add_prob(NA, chn), add_prob(substring(cor, pos, pos), chn)))
    }
    
    if(typ=='d'){
      s <- as.numeric(ifelse(pos==0, del_prob(NA, chn), del_prob(substring(cor, pos, pos), chn)))
    }
    
    if(typ=='s'){
      s <- as.numeric(sub_prob(substring(t, pos+1, pos+1), chn))
    }
    
    if(typ=='r'){
      s <- as.numeric(rev_prob(substring(cor, pos+1, pos+1), substring(cor, pos+2, pos+2)))
    }
    
    pr <- prior(cor)
    
    correction <- c(correction, cor)
    scr <- c(scr, s)
    pc <- c(pc, pr)
  }
  rk <- data.frame(typo = rep(typo, nrow(list_cand)), correction= correction, pc = pc, ptc = scr) %>% dplyr::arrange(desc(ptc))
  return(rk)
}

##Step7: Save Results
load("../output/candidates_gd.RData")
unique_cand <- unique(candidates_gd$typo)
unique_cand_list <- as.list(unique_cand)
cand_prob_wt_context <- lapply(unique_cand_list, prob_t_c_ap)
cand_prob_wt_context_df <-ldply(cand_prob_wt_context,data.frame)
save(cand_prob_wt_context_df,file = "../output/cand_prob_wt_cont_gd.RData")
```


####Step 2: With context (Section 5 in assigned paper)
First, we need to count up the 2-gram in the `ground_truth` files.  
```{r, eval=FALSE}
## paste all text
file_name_vec <- list.files("../data/ground_truth")
gd_all <- c()
for (i in 1:length(file_name_vec)){
  current_file_name <- sub(".txt","",file_name_vec[i])
  gd_lines <- readLines(paste("../data/ground_truth/",current_file_name,".txt",sep=""), warn=FALSE) %>%
  gd_lines <- paste(str_trim(gd_lines), sep = " ", collapse = " ")
  gd_all <- paste(gd_all,gd_lines)
}
save(gd_all,file = '../output/gd_all.RData')

## deal with punctuation and paste all text
source('../lib/cleantext.R')
all_text = cleantext(gd_all)
all_text = paste("NULL ",all_text)
save(all_text,file = '../output/all_text.RData')

## calculate context frequency

ng <- ngram (all_text, n =  2)
bigrams = data.frame(get.phrasetable(ng))

bigram_counts = bigrams %>%
  separate(ngrams, c("l", "r"), sep = " ") %>%
  .[,c("l","r","freq")]

## GT turning
r=table(bigram_counts$freq)
r_star = c()
for (i in 1:(length(r)-1)) {
  r_star = c(r_star,(as.integer(names(r[i]))+1)*r[i+1]/r[i])
}
r_star = c(r_star, as.integer(names(r[length(r)]))+1)
names(r_star) = names(r)
f = c()
for (j in 1:nrow(bigram_counts)) {
  f = c(f,r_star[as.character(bigram_counts[j,]$freq)])
}

bigram_counts = cbind(bigram_counts,f)
save(bigram_counts,file = '../output/bigram_counts.RData')
```

And then, we will calculate $P(l|c)$ and $P(r|c)$ for each possible corrected word.   
```{r, eval=FALSE}
load(../output/cand_prob_wt_cont_gd.RData)
load(../output/typos.RData)
source(../lib/get_lr.R)
source(../context_prob.R)

## merge
typos$typo=tolower(typos$typo) %>% gsub("[[:punct:]]","",.)
final_prob = merge(x=cand_prob_wt_context_df,y=typos,by="typo",all.x=TRUE) %>%
  .[which(is.na(.$posx)==FALSE),]

## Find out the left and right word in gound_truth files
a = t(apply(final_prob[,c(5,6)], 1, get_lr))

## Calculate P(l|c) and P(r|c)
p = t(apply(final_prob[,c(5,2,6)], 1, context_prob))
final_prob = cbind(final_prob,p)
final_prob = final_prob[,c(1,2,5,6,3,4,7,8)]
colnames(final_prob)=c("typo","correction","posx","posy","pc","ptc","pl|c","pr|c")
save(final_prob,file = '../output/final_prob.RData')
```


# Step 5 - Performance measure

The two most common OCR accuracy measures are precision and recall. Both are relative measures of the OCR accuracy because they are computed as ratios of the correct output to the total output (precision) or input (recall). More formally defined,
\begin{align*}
\mbox{precision}&=\frac{\mbox{number of correct items}}{\mbox{number of items in OCR output}}\\
\mbox{recall}&=\frac{\mbox{number of correct items}}{\mbox{number of items in ground truth}}
\end{align*}
where *items* refer to either characters or words, and ground truth is the original text stored in the plain text file. 

Both *precision* and *recall* are mathematically convenient measures because their numeric values are some decimal fractions in the range between 0.0 and 1.0, and thus can be written as percentages. For instance, recall is the percentage of words in the original text correctly found by the OCR engine, whereas precision is the percentage of correctly found words with respect to the total word count of the OCR output. Note that in the OCR-related literature, the term OCR accuracy often refers to recall.

Here, we only finished the **word level evaluation** criterions, you are required to complete the **character-level** part.

```{r}
load('../output/gd_all.RData')
load('../output/tesseract_vec.RData')
load('../output/tesseract_vec_correct.RData')
## Here, we compare the lower case version of the tokens
ground_truth_vec <- strsplit(gd_all," ")[[1]]%>%
  sapply(.,tolower)%>%
  as.vector()%>%
  gsub('[[:punct:] ]+','',.)%>%
  .[.!=""]

tesseract_vec <- unlist(tesseract_vec)%>%
  sapply(.,tolower)%>%
  as.vector()%>%
  gsub('[[:punct:] ]+','',.)%>%
  .[.!=""]

tesseract_delete_error_vec <- unlist(tesseract_vec_new," ")%>%
  sapply(.,tolower)%>%
  as.vector()%>%
  gsub('[[:punct:] ]+','',.)%>%
  .[.!=""]

## word level
old_intersect_vec <- vecsets::vintersect(ground_truth_vec,tesseract_vec)
new_intersect_vec <- vecsets::vintersect(ground_truth_vec,tesseract_delete_error_vec)

## character level
GT_character_vec <- unlist(strsplit(ground_truth_vec,""))
T_character_vec <- unlist(strsplit(tesseract_vec,""))
T_deleted_character_vec <- unlist(strsplit(tesseract_delete_error_vec,""))

old_intersect_character <- vecsets::vintersect(GT_character_vec,T_character_vec)
new_intersect_character <- vecsets::vintersect(GT_character_vec,T_deleted_character_vec)


## Table
OCR_performance_table <- data.frame("Tesseract" = rep(NA,4),
                                    "Tesseract_with_postprocessing" = rep(NA,4))
row.names(OCR_performance_table) <- c("word_wise_recall","word_wise_precision",          "character_wise_recall","character_wise_precision")

OCR_performance_table["word_wise_recall","Tesseract"] <- length(old_intersect_vec)/length(ground_truth_vec)
OCR_performance_table["word_wise_precision","Tesseract"] <- length(old_intersect_vec)/length(tesseract_vec)
OCR_performance_table["word_wise_recall","Tesseract_with_postprocessing"] <- length(new_intersect_vec)/length(ground_truth_vec)
OCR_performance_table["word_wise_precision","Tesseract_with_postprocessing"] <- length(new_intersect_vec)/length(tesseract_delete_error_vec)

OCR_performance_table["character_wise_recall","Tesseract"] <- length(old_intersect_character)/length(GT_character_vec)
OCR_performance_table["character_wise_precision","Tesseract"] <- length(old_intersect_character)/length(T_character_vec)
OCR_performance_table["character_wise_recall","Tesseract_with_postprocessing"] <- length(new_intersect_character)/length(GT_character_vec)
OCR_performance_table["character_wise_precision","Tesseract_with_postprocessing"] <- length(new_intersect_character)/length(T_deleted_character_vec)
```

Demonstrating Final Result: `Teseeract` vs. `Tesseract_with_postprocessing`:

```{r}
kable(OCR_performance_table, caption="Summary of OCR performance")
```

Besides the above required measurement, you are encouraged to explore more evaluation measurements. Here are some related references:

1. Karpinski, R., Lohani, D., & BelaÃ¯d, A. *Metrics for Complete Evaluation of OCR Performance*. [pdf](https://csce.ucmss.com/cr/books/2018/LFS/CSREA2018/IPC3481.pdf)

- section 2.1 Text-to-Text evaluation

2. Mei, J., Islam, A., Wu, Y., Moh'd, A., & Milios, E. E. (2016). *Statistical learning for OCR text correction*. arXiv preprint arXiv:1611.06950. [pdf](https://arxiv.org/pdf/1611.06950.pdf)

- section 5, separate the error detection and correction criterions

3. Belaid, A., & Pierron, L. (2001, December). *Generic approach for OCR performance evaluation*. In Document Recognition and Retrieval IX (Vol. 4670, pp. 203-216). International Society for Optics and Photonics. [pdf](https://members.loria.fr/ABelaid/publis/spie02-belaid-pierron.pdf)

- section 3.2, consider the text alignment

# References {-}

1. Borovikov, E. (2014). *A survey of modern optical character recognition techniques*. arXiv preprint arXiv:1412.4183.[pdf](https://pdfs.semanticscholar.org/79c9/cc90b8c2e2c9c54c3862935ea00df7dd56ed.pdf)
(This paper is the source of our evaluation criterion)

2. Kukich, K. (1992). *Techniques for automatically correcting words in text*. Acm Computing Surveys (CSUR), 24(4), 377-439. [pdf](http://www.unige.ch/eti/ptt/docs/kukich-92.pdf)
(This paper is the benchmark review paper)
